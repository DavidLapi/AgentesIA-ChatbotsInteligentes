# Basic IA Agent with memory â¬‡ï¸â¬‡ï¸â¬‡ï¸

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import PromptTemplate
from langchain_ollama import OllamaLLM

# Load AI Model from Ollama
llm = OllamaLLM(model="mistral")

# Inicialize Memory
chat_history = ChatMessageHistory() # Stores user-AI conversation history

# Define AI Chat Prompt
prompt = PromptTemplate(
    imput_variables=["chat_history", "question"],
    template="Previous conversation: {chat_history}\nUser: {question}\nAI:"
)

# Function to run AI chat with memory
def run_chain(question):
    # Retrieve chat history manually
    chat_history_text = "\n".join([f"{msg.type.capitalize()}: {msg.content}" for msg in chat_history.messages])

    # RUN the AI response generation
    response = llm.invoke(prompt.format(chat_history=chat_history_text, question=question))

    # Store new user input and AI response in memory
    chat_history.add_user_message(question)
    chat_history.add_ai_message(response)

    return response

# Interactive CLI Chatbot
print("\nğŸ“¤ AI Chatbot with Memory")
print("Type 'exit' to stop.")

while True:
    user_input = input("\nğŸ“‹ You: ")
    if user_input.lower() == "exit":
        print("\nğŸ‘‹ Good Bye!!")
        break

    ai_response = run_chain(user_input)
    print(f"\nğŸ“¨ AI: {ai_response}")