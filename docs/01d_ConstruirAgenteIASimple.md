# Construir un Agente de IA Simple

Ahora que tenemos todo nuestro entorno configurado, vamos a construir **un agente de IA simple**.

Aqu√≠ vamos a construir un **chatbot** impulsado por IA usando **LangChain + Ollama**.

1. El primer paso ser√° instalar los paquetes necesarios.
2. Luego procederemos a crear un archivo Python que se llamar√° *basic_ai_agent.py*.

## Empecemos

Accedemos a nuestro VS Code y nos dirigimos a nuestra ruta: C:/Users/alumno/Desktop/AIAgents$

Crearemos un directorio llamada "Day1". Dentro de ese directorio crearemos un nuevo archivo. Lo llamaremos *basic_ai_agent.py*.

Ahora, nuestro primer paso mencionado anteriormente es instalar los paquetes necesarios. As√≠ que accederemos a la terminal de VS Code:

```bash
PS C:\Users\alumno\Desktop\AIAgents>
```

Para comprobar de que est√° instalado Python, comprobaremos los siguientes datos:

```bash
PS C:\Users\alumno\Desktop\AIAgents> python --version
Python 3.13.4
# Si nos sale la versi√≥n de Python, es que se ha instalado correctamente.
# Vamos a comprobar en qu√© ruta estamos
PS C:\Users\alumno\Desktop\AIAgents> pwd

Path
----
C:\Users\alumno\Desktop\AIAgents

# Accedemos a nuestro directorio Day1 y comprobamos si est√° el archivo creado.
PS C:\Users\alumno\Desktop\AIAgents> cd Day1
PS C:\Users\alumno\Desktop\AIAgents\Day1> ls

Directorio: C:\Users\alumno\Desktop\AIAgents\Day1

Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a----        27/01/2026     11:15              0 basic_ai_agent.py

# Limpiamos la pantalla de la terminal con cls
PS C:\Users\alumno\Desktop\AIAgents\Day1> cls
```

## Instalar paquetes

```bash
# Ejecutamos el comando: pip install langchain langchain-community langchain-ollama
PS C:\Users\alumno\Desktop\AIAgents\Day1> pip install langchain langchain-community langchain-ollama
```

## ¬øQu√© significan estos comandos?

langchain, langchain-community, langchain-ollama --> Estas son las bibliotecas necesarias que puedes usar.

## El comando "pip"

El comando pip es algo que viene con Python. Si no est√° all√≠, probablemente tendr√°s que proceder a instalarlo. Para comprobar si est√° instalado, ejecuta el siguiente comando:
```bash
pip --version
# Resultado:
pip 25.1.1 from C:\Users\alumno\AppData\Local\Programs\Python\Python313\Lib\site-packages\pip (python 3.13)
```

Puedes abrir la terminal y simplemente decir, **pip, instalar langchain**; y LangChain ser√° descargado e instalado.

## LangChain

Si nunca has o√≠do hablar de LangChain, la funcionalidad principal te ayuda para construir aplicaciones basadas en **LLM (Large Language Model o Modelo de Lenguaje Grande)**. Proporciona componentes como cadenas para comibinar diferentes funcionalidades de LLM. 

Tambi√©n admite la integraci√≥n con varios proveedores de LLM como **OpenAI y Huggingface**, y por eso estamos instalando LangChain.

## LangChain Community

**LangChain Community** es una adicional desarrollada por la comunidad de LangChain con nuevas caracter√≠sticas y funcionalidades. 

Se sigue actualizando, as√≠ que es bueno mantenerlo. Quiere ecir, instalarlo cada vez solo para que si hay nuevas caracter√≠sticas o algo nuevo que est√©s usando est√© disponible para t√≠.

Y a menudo incluye caracter√≠sticas experimentales o soporte para casos de uso espec√≠ficos, por lo que es muy √∫til.

## LangChain Ollama

Esta biblioteca permite a los usuarios aprovechar Ollama para ejecutar LLM de c√≥digo abierto localmente dentro de sus aplicaciones de linterna

Permite **privacidad y control** sobre los datos ejecutando modelos en nuestra m√°quina local,as√≠ que eso es lo que estaremos haciendo.

Vamos a proceder a ejecutar el comando **pip install langchain langchain-community langchain-ollama**. Saldr√° un mont√≥n de dependencias a descargar e instalar, y nos llevar√° unos minutos a proceder el comando. Si sale alg√∫n error, hazmelo saber para solucionarlo.

## Agente de IA b√°sico

Nuestro siguiente paso es escribir el c√≥digo en nuestro archivo (basic_ai_agent.py) para nuestro agente de IA b√°sico.

Empezaremos por importar Ollama LLM, as√≠ que en la primera l√≠nea de c√≥digo escribiremos lo siguiente:

```py
from langchain_ollama import OllamaLLM
```

Lo que hace esta primera l√≠nea es que est√° importando la clase **"OllamaLLM"** del paquete **"langchain_ollama"** que instalamos previamente.

**OllamaLLM** es una implementaci√≥n de un modelo de lenguaje que utiliza Ollama, un marco para ejecutar e interactuar con modelos de IA que ya hemos instalado y est√° funcionando en nuestra m√°quina.

Ahora, esto nos permite interactuar con modelos de IA como Mistral que hemos instalado. Tambi√©n puedes usar la matriz u otros a trav√©s de LangChain para que est√© disponible para nosotros.

En la siguiente l√≠nea escribiremos lo siguiente:

```py
# Load AI Model from Ollama
llm = OllamaLLM(model="mistral")
```

La variable "llm" inicializa el modelo de IA. El modelo *OllamaLLM(model="mistral")* crea una instancia de la clase LLM y la asigna a la variable LLM. Ahora Mistral especifica el modelo que se est√° utlizando. Puedes reemplazarlo por "deepseek-r1" o "gemma" o cualquier otro modelo que desees.

Ahora, el objeto "llm" se usar√° m√°s tarde para generar respuestas basadas en la entrada del usuario, as√≠ que tenlo en cuenta.

Escribiremos la siguiente l√≠nea:

```py
print("\nBienvenido a tu Agente de IA! Preguntame algo...")
```

Este es un mensaje que le vamos a mostrar al usuario en la CLI. As√≠ que eso se ejecutar√°.

Una vez escrito el mensaje, escribiremos lo siguiente:

```py
while True:
    question = input("Make your question (or write 'exit' to stop the machine): ")
    if question.lower() == "exit":
        print("Good bye, begginner!")
        break
    response = llm.invoke(question)
    print("\n Response for IA: ", response)
```

1. **while True:**

Ejecutaremos un bucle "while". Esto hace que, mientras sea verdadero (True), se ejecutar√° constantemente y har√° preguntas al usuario.

2. **question = input("Make your question (or write 'exit' to stop the machine): ")**

Dentro del bucle "while", creamos una variable de pregunta (question) y a√±adimos un "input" para tomar la entrada al usuario. Es decir, que despu√©s de mostrar el mensaje, esperar√° a que el usuario especifique algo. El usuario tiene dos opciones, de escribir una pregunta o escribe 'exit' para detener el bucle "while".

3. **if question.lower() == "exit":**

Aqu√≠ se verifica una funci√≥n ifelse por si ve la palabra "exit". Si por un casual la m√°quina ve "Exit" con alguna may√∫scula de por medio, la variable lo convierte en min√∫scula autom√°ticamente con la funci√≥n *lower()*.

4. **print("Good bye, begginner!")**

Muestra el mensaje de despedida al escribir la palabra "exit".

5. **break**

Despu√©s del mensaje de despedida, se termina el bucle con la condici√≥n "break". Esto cierra la aplicaci√≥n.

6. **response = llm.invoke(question)**

Si el usuario escribi√≥ una pregunta, se declara una variable "response" que se encarga de hacer que la variable "llm" pueda invocar (invoke) una respuesta del modelo de IA al usuario. As√≠ que "question" se pasar√° dentro de la funcion "invoke". Esta l√≠nea realmente pasa la pregunta al modelo de IA y genera y obtiene una respuesta del modelo. As√≠ que la respuesta se almacena de nuevo en la variable "response".

7. **print("\n Response for IA: ", response)**

Una vez declarado response, hay que imprimir la respuesta de la IA. Y con esto ser√≠a todo.

## Prueba

Ya tenemos nuestro agente de IA funcionando. Ahora lleg√≥ la hora de ejecutar el archivo para comprobar si funciona. Abrimos la terminal y nos dirigimos a nuestra ruta con el archivo almacenado. Una vez dentro, ejecutaremos el siguiente comando:

```bash
C:\Users\alumno\Desktop\AIAgents\Day1> py .\basic_ai_agent.py
# Nos saldr√≠a un resultado as√≠:

Welcome to your Agent IA! Ask me something...
Make your question (or write 'exit' to stop the machine): 

# Le haremos una pregunta en ingl√©s para empezar, porque el modelo instalado no es multiidioma.
# Le preguntaremos algo como: Qu√© es IA?
# Nos saldr√≠a una respuesta como esta:
# ---
# Response for IA:   IA, or Intelligent Assistants, refer to computer systems and software designed to perform tasks for human users. These tasks can range from answering questions, making recommendations, scheduling appointments, sending messages, or even controlling other applications. Examples of Intelligent Assistants include virtual assistants like Siri, Alexa, Google Assistant, and Cortana. They are powered by artificial intelligence (AI), natural language processing (NLP), and machine learning (ML) technologies to understand user requests and respond in a conversational manner. The goal is to make interactions with technology more seamless, intuitive, and human-like.
# ---

```

# Y listo ü•≥ 

Ya tenemos nuestro primer chatbot b√°sico para poder aprender.
