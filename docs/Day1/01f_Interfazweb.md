# Crear una Interfaz Web para el Agente de IA

El pr√≥ximo tema que quiero abordar es la **creaci√≥n de una interfaz web (Web User Interface)** para un agente de IA.

Ya hemos creado una interfaz de l√≠nea de comandos, o cual es genial, pero vamos a a√±adir una interfaz web utilizando una biblioteca llamada **Streamlit**. Hace todo por t√≠ y es muy √∫til.

Primero lo que vamos a hacer es **instalar Streamlit**, as√≠ que nos dirigimos a la terminal en nuestra ruta:

```bash
C:\Users\alumno\Desktop\AIAgents\Day1>
```

Para instalar la biblioteca, ejecutaremos el siguiente comando:

```bash
pip install streamlit
```

Puedes leer m√°s de Streamlit a trav√©s de este enlace --> [docs.stremlit.io](https://docs.streamlit.io/).

## Archivo basic_ai_agent.py

Una vez que hemos instalado la biblioteca, vamos a **crear una interfaz web**. As√≠ que empecemos:

1. Vamos a volver a la misma aplicaci√≥n que hemos creado, que es nuestro agente de IA b√°sico, y vamos a ir hasta la parte superior para aqu√≠ de nuevo comentarlo todo.

Si se prefiere, se puede a√±adir de esta manera:

```py
# Agente de IA B√°sico ‚¨áÔ∏è‚¨áÔ∏è

# from langchain_ollama import OllamaLLM

# Load AI Model from Ollama
# llm = OllamaLLM(model="mistral")

# print("\nWelcome to your Agent IA! Ask me something...")

# while True:
    # question = input("Make your question (or write 'exit' to stop the machine): ")
    # if question.lower() == "exit":
        # print("Good bye, begginner!")
        # break
    # response = llm.invoke(question)
    # print("\n Response for IA: ", response)

# ---------------------------------------------------------------------------------------

# Agente de IA B√°sico Con Memoria ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è

# from langchain_community.chat_message_histories import ChatMessageHistory
# from langchain_core.prompts import PromptTemplate
# from langchain_ollama import OllamaLLM

# Load AI Model from Ollama
# llm = OllamaLLM(model="mistral") # Change to "llama3" or another Ollama model

# Inicialize Memory
# chat_history = ChatMessageHistory() # Stores user-AI conversation history

# Define AI Chat Prompt
# prompt = PromptTemplate(
    # imput_variables=["chat_history", "question"],
    # template="Previous conversation: {chat_history}\nUser: {question}\nAI:"
# )

# Function to run AI chat with memory
# def run_chain(question):
    # Retrieve chat history manually
    # chat_history_text = "\n".join([f"{msg.type.capitalize()}: {msg.content}" for msg in chat_history.messages])

    # RUN the AI response generation
    # response = llm.invoke(prompt.format(chat_history=chat_history_text, question=question))

    # Store new user input and AI response in memory
    # chat_history.add_user_message(question)
    # chat_history.add_ai_message(response)

    # return response

# Interactive CLI Chatbot
# print("\nüì§ AI Chatbot with Memory")
# print("Type 'exit' to stop.")

# while True:
    # user_input = input("\nüìã You: ")
    # if user_input.lower() == "exit":
        # print("\nüëã Good Bye!!")
        # break

    # ai_response = run_chain(user_input)
    # print(f"\nüì® AI: {ai_response}")

# Agente de IA con Interfaz Web (Aqu√≠ es donde empezar√≠a el c√≥digo) ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è

```

Guarda el archivo, y una vez hecho importaremos la biblioteca:

```py
import streamlit as st
```

Este es Streamlit, una biblioteca de Python para crear aplicaciones basadas en la web r√°pidamente. Se utiliza aqu√≠ para construir una interfaz de chatbot de IA simple, por eso lo estoy importando.

A continuaci√≥n, voy a importar el paquete de *historial de mensajes por chat* desde **LangChain Community**:

```py
from langchain_community.chat_message_histories import ChatMessageHistory
```

Nuevamente, esto importa el historial de mensajes de chat, lo cual ayuda a almacenar y gestionar el historial de chat entre el usuario y la IA.

A continuaci√≥n importamos los dos paquetes que usamos recientemente para la otra versi√≥n del agente de IA:

```py
from langchain_core.prompts import PromptTemplate # <-- Para la plantilla del prompt
from langchain_ollama import OllamaLLM # <-- Para conectar con Ollama desd LangChain
```

A continuaci√≥n, cargar√© el modelo de IA:

```py
# Load AI Model from Ollama
llm = OllamaLLM(model="mistral") # Change to "llama3" or another Ollama model
```

Ahora vamos a proceder a inicializar la memoria:

```py
# Inicialize Memory
if "chat_history" not in st.session_state:
    st.session_state.chat_history = ChatMessageHistory() # Stores user-AI conversation history 
```

Esto almacena el historial de conversaci√≥n del usuario con la IA.

¬øPor qu√© la condici√≥n if? Esto verifica si el historial de chat existe en el estado de sesi√≥n de Streamlit. Si el historial de chat no est√° ya configurado, inicializa el historial de chat aqu√≠. Luego con la variable "chat_history" permite que el chatbot recuerde conversaciones pasadas.

A continuaci√≥n procedemos a definir el chat de IA, empezando por la plantilla:

```py
# Define AI Chat Prompt
prompt = PromptTemplate(
    imput_variables=["chat_history", "question"],
    template="Previous conversation: {chat_history}\nUser: {question}\nAI:"
)
```

Esto nuevamente define c√≥mo se env√≠an los mensajes a la IA.

Vamos a definir la funci√≥n para ejecutar el chat de IA con memoria. Esto ser√≠a exactamente la misma funci√≥n que tenemos antes:

```py
# Function to run AI chat with memory
def run_chain(question):
```

Pero claro, aqu√≠ debemos de modificar la variable "chat_history_text" para que haga conexi√≥n con Streamlit. Entonces modificar√≠amos esta parte:

```py
    # Retrieve chat history manually (st.session_state)
    chat_history_text = "\n".join([f"{msg.type.capitalize()}: {msg.content}" for msg in st.session_state.chat_history.messages])
```

Habr√≠a que modificar el bucle for para tomar el punto "st.session_state". A diferencia del otro agente, antes tomaba los mensajes de "chat_history.messages", pero en este caso, estoy diciendo "st.session_state.chat_history.messages".

A continuaci√≥n vamos a proceder a ejecutar el generador de respuestas de IA "llm":

```py
    # RUN the AI response generation
    response = llm.invoke(prompt.format(chat_history=chat_history_text, question=question))
```

Ahora a almacenar la nueva entrada del usuario y la respuesta de la IA en la memoria, pero usando Streamlit (st.session_state):

```py
    # Store new user input and AI response in memory
    st.session_state.chat_history.add_user_message(question)
    st.session_state.chat_history.add_ai_message(response)
```

Y luego devuelve la respuesta de la funci√≥n run_chain:

```py
    return response
```

---

Esto va a ser nuevo para nosotros, as√≠ que primero vamos a crear primero el t√≠tulo con Streamlit:

```py
# Streamlit UI
st.title("ü§ñ AI Chatbot with Memory ")
```

Este ser√° el t√≠tulo que mostramos de manera prominente. A continuaci√≥n voy a escribir:

```py
st.write("Ask me anything!")
```

As√≠ que esto se mostrar√° el mensaje "Preg√∫ntame lo que sea!".

A continuaci√≥n, vamos a crear algo para la entrada del usuario:

```py
user_input = st.text_input("üìã Your Question:")
```

Esto crear√° un campo de entrada de texto para que el usuario escriba su pregunta. As√≠ que cuando abras el navegador, ver√°s de qu√© estoy hablando.

Siguiente proceso, a procesar la entrada del usuario:

```py
if user_input:
    response = run_chain(user_input)
```

Ahora, si la respuesta de entrada del usuario es igual a ejecutar cada de entrada del usuario, se le llama a la funci√≥n, obtiene la respuesta y dir√° lo siguiente:

```py
    # Dentro de if
    st.write(f"**You:** {user_input}")
    st.write(f"**Ai:** {response}")
```

La IA di√≥ esta respuesta. Aahora, si el usuario ingresa, una pregunta pasar√° por esto, ejecutar√° la cadena y la enviar√° a la IA para obtener una respuesta. 

Las preguntas del usuario y la respuesta de la IA se muestran en la interfaz web utilizando estos mensajes.

A continuaci√≥n, vamos a seguir adelante y mostrar el historial completo del chat:

```py
# Show full chat history
st.subheader("üìú Chat History")
```

Esto muestra el cabecero del historial completo del chat en la interfaz. Lo siguiente es un bucle para recorrer el historial del chat e imprimir cada mensaje. As√≠ que voy a decir:

```py
for msg in st.session_state.chat_history.messages:
    st.write(f"**{msg.type.capitalize()}**: {msg.content}")
```

As√≠ que esa es toda nuestra aplicaci√≥n. Lo que hace es nuevamente crear una aplicaci√≥n web con Streamlit donde los usuarios pueden chatear con la IA. La IA recuerda conversaciones pasadas que se almacenan en el estado de sesi√≥n y muestra el historial completo de la conversaci√≥n de manera din√°mica.

## Prueba

Nos volvemos a la terminal con nuestra ruta que tenga el archivo del agente de IA y ejecutaremos el siguiente comando:

```bash
streamlit run basic_ai_agent.py
```

Nos muestra en la terminal para abrir una URL como as√≠:

![RespuestaIA](./img/ResponseStreamlit_1.png)

Una vez ejecutado el comando nos aparece por defecto esta ventana en el navegador:

![RespuestaIA](./img/ResponseStreamlit_2.png)

Si no se abre, puedes simplemente ir a la terminal y copiar la URL local (http://localhost:8501) para luego abrirlo en el navegador, y te mostrar√° lo mismo nuevamente.

Ahora como puedes ver, el t√≠tulo que hemos espicificado antes (st.title), que es "AI Chatbot with Memory" se muestra en pantalla. Y luego tambi√©n "Ask me anything!" desde la l√≠nea despu√©s del t√≠tulo (st.write) aparece en pantalla.

"üìã Your Question:" tambi√©n se muestra como cabecero del input para especificar una pregunta.

El historial de chat est√° ah√≠, pero no hay chat. Por eso est√° vac√≠o. Voy a preguntar: "What is IA Agent?". Veamos que ocurre:

![RespuestaIA](./img/ResponseStreamlit_3.png)

As√≠ ya vemos que est√° funcionando y me da la respuesta, y tambi√©n el historial. ¬øQu√© dijo el humano (Human)? ¬øQu√© dijo la IA?

Ahora vamos a hacer otra pregunta: "Can it learn for itself?". Veamos qu√© ocurre:

![RespuestaIA](./img/ResponseStreamlit_4.png)

Vemos que aqu√≠ ya lo est√° almacenando todo en el espacio del historial. Ahora vamos a preguntarle: "What is the first question I asked?" Veamos qu√© dice:

![RespuestaIA](./img/ResponseStreamlit_5.png)

Como puedes ver, se almacena la respuesta en el historial y lo recuerda.

Y con esto, ya tienes tu chatbot completo con memoria disponible en un sitio web. As√≠ que creamos un chatbot basado en la web usando Streamlit. Prueba esto y d√©jame saber c√≥mo te va.

Si tienes alguna pregunta o tuviste alg√∫n problema al implementarlo, d√©jame saber en los comentarios.

Y con esto se finaliza el d√≠a 1, as√≠ que espero que hayas disfrutado de este primer d√≠a. Nos veremos en el d√≠a 2. üëã

Anterior p√°gina: Agregar Memoria --> [**Click aqu√≠**](./01e_AgregarMemoria.md)

Pasar al d√≠a 2 --> [**Click aqu√≠**](../Day2/02_Intro.md)