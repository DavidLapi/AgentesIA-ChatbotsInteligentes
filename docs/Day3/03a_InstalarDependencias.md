# Instalar dependencias

Nuestro primer paso, como siempre, es instalar las dependencias necesarias.

Antes de empezar a programar, veamos todas las diferentes bibliotecas que necesitamos:

- **requests** --> Obtiene datos de páginas web.

- **beautifulsoup4** --> Extrae y limpia texto de HTML.

- **langchain_ollama** --> Usa Ollama para el *resumen impulsado con IA* (AI-powered summarization).

- **streamlit** --> Construye una interfaz web para el *raspador web* (web scrapper).

## Comencemos

Accedemos a nuestra terminal en una ruta habitual:

*C:\Users\alumno\Desktop\AIAgents>*

Desde nuestra ruta crearemos nuestra carpeta llamada Day3 (no accedamos a ella aún).

Desde nuestra ruta empezaremos a crear nuestra dependencias ya mencionadas. Nuestro comando para ejecutar sería:

```bash
pip install requests beautifulsoup4 langchain_ollama streamlit
```

Esperemos unos minutos a que instalen las dependencias. Y una vez que termine, vamos a proceder a limpiar la terminal:

```bash
cls # o 'clear' también sirve
```

Una vez hecho, accedemos a la carpeta creada "Day3" y crearemos el archivo, pero lo dejaremos para más tarde.

## Listo!

Así que una vez que estén instaladas, estarás listo para construir nuestro raspador web de IA.

Anterior página: Intro --> [**Click aquí**](./03_Intro.md)

Siguiente página: Web Scrapper--> [**Click aquí**](./03b_WebScraper.md)