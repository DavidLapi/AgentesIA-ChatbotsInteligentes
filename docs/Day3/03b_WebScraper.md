# Construir el Web Scraper de IA

Empecemos con la construcci√≥n del **raspador web de IA**. Vamos a crear un **script** de Python que:

- Busque en sitios web.

- Extraiga y limpie texto.

- Use IA para resumir el contenido.

## Script de Python

As√≠ que vamos a crear nuestro archivo de c√≥digo. Volvemos a la terminal:

*C:\Users\alumno\Desktop\AIAgents>*

Ya hemos creado nuestra carpeta "Day3", as√≠ que nos desplazaremos a esa carpeta donde crearemos nuestro archivo:

```bash
cd Day3
```

Dentro crearemos nuestro archivo y lo guardamos como nombre "ai_web_scraper.py".

## Importaci√≥n de bibliotecas instaladas

Una vez creado, ya podemos empezar a escribir c√≥digo. Empezaremos por *importar las bibliotecas* que hemos instalado anteriormente:

```py
import requests
from bs4 import BeautifulSoup
import streamlit as st
from langchain_ollama import OllamaLLM
```

## Repaso r√°pido de bibliotecas

- **request**: se utiliza para hacer solicitudes HTTP para obetener p√°ginas web.

- **beautifulsoup4**: analiza HTML y extrae texto de p√°ginas web.

- **streamlit**: construye aplicaciones web interactivas como interfaces web de usuario (Web UI).

- **langchain_ollama**: se utiliza para interactuar con un modelo de lenguaje de IA (*mistral*, *llama3*, *deepseek*...).

## Cargar Modelo LLM

Ahora procederemos a cargar el modelo de IA y usar√© el modelo "Mistral":

```py
# Load AI Model
llm = OllamaLLM(model="mistral") # Change to "llama3" or another Ollama model
```

Esta instancia de LLM se utilizar√° para generar respuestas de texto, como resumir el contenido de sitios web en este contexto particular.

Puedes reemplazar el modelo "mistral" con la matriz "deepseek", "llama3" o cualquier otro modelo que Ollama soporte y que pueda usarse junto con Ollama LLM.

## Funci√≥n para extraer datos de un sitio web

A continuaci√≥n, vamos a escribir una funci√≥n para extraer datos de un sitio web:

```py
# Function to scrape a website
def scrape_website(url): #1
    try: #2
        st.write(f"üåç Scraping website: {url}") #3
        headers = {"User-Agent": "Mozilla/5.0"} #4
        response = requests.get(url, headers=headers) #5

        if response.status_code != 200: #6
            return f"‚ò£Ô∏è Failed to fetch {url}" #7

        # Extract text content 
        soup = BeautifulSoup(response.text, "html.parser") #8
        paragraphs = soup.find_all("p") #9
        text = " ".join([p.get_text() for p in paragraphs]) #10

        return text[:2000] #11
    except Exception as e: #12
        return f"‚ùå Error: {str(e)}" #13
```

1. Esta funci√≥n se llama "scrape_website", que sirve para tomar una URL como entrada y extrae su contenido.

2. Intentaremos ejecutar un c√≥digo si no nos produce un error.

3. Esto muestra un mensaje en la interfaz de usuario de Streamlit para informar al usuario que se est√° extrayendo el sitio web.

4. Establecen un encabezado de agente de usuario para imitar una solicitud de navegador real.

5. El "requests.get()" env√≠a una solicitud GET para obtener la p√°gina web de esa url o vino especificada.

6. Si verificas si la solicitud fue exitosa, el statuscode 200 significa que fue exitoso. Si no es 200, eso significa que algo no sali√≥ bien.

7. Entonces se devuelve la respuesta de que no se pudo obtener la URL especificada.

8. Esto analiza el contenido HTML que obtuvimos de la p√°gina web usando *BeautifulSoup*.

9. Lo que hace es encontrar todas las etiquetas HTML o etiquetas de p√°rrafo "p" en la p√°gina web.

10. La variable "text" hace que se unan todos los p√°rrafos juntos separados por un espacio. Nuevamente, "text" extrae el texto de cada p√°rrafo y los une una sola cadena.

11. Devuelve el texto anterior, pero lo limita a 2000 caracteres (*:2000*) para evitar que sobrecargue el modelo de IA. Si se quiere aumentar m√°s, puedes aumentar m√°s si lo deseas.

12. Verificamos las excepciones.

13. Si ocurre un error, captura la excepci√≥n y devuelve un mensaje de error.

## Funci√≥n para resumir contenido usando IA

A continuaci√≥n, voy a proceder a escribir una funci√≥n para resumir contenido con IA:

```py
# Function to summarize content using AI
def summarize_content(content): #1
    st.write("üñäÔ∏è Summarizing content...") #2
    return llm.invoke(f"Summarize the following content:\n\n{content[:1000]}") #3
```

1. Esta funci√≥n se llama "summarize_content", que sirve para generar un resumen del texto extra√≠do.

2. Esto muestra un mensaje resumiendo el contenido en la interfaz de usuario de Streamlit.

3. Lo que hace es llamar al modelo de IA para resumir el texto. El texto de entreada est√° limitado a 1000 caracteres para evitar sobrecargar de nuevo la IA. 

## Steamlit Web UI

A continuaci√≥n, vamos a hacer el dise√±o de la interfaz:

```py
# Streamlit Web UI
st.title("ü§ñ AI-Powered Web Scraper")
st.write("Enter a website URL below and get a summarized version!")
```

Establecemos primero el t√≠tulo de la aplicaci√≥n Streamlit "st.title" como el raspador web impulsado por IA. Luego muestra instrucciones "st.write" sobre c√≥mo usar la aplicaci√≥n.

## Entrada del usuario

A continuaci√≥n creamos la entrada del usuario. Tomar√© la entrada del usuario para la URL del sitio web:

```py
# User input
url = st.text_input("‚õìÔ∏è‚Äçüí• Enter Website URL:")
```

Esto crea nuevamente un cuadro de entrada donde el usuario puede ingresar una URL de sitio web. Despu√©s crearemos la siguiente condici√≥n "if":

```py
if url:
    content = scrape_website(url)
```

Si el usuario ha ingresado una URL adecuada, llama a la funci√≥n "scrape_website" para que, de nuevo, extraiga los datos del sitio web.

Pero si hay un fallo en el contenido o un error en el contenido, escribiremos lo siguiente dentro del if:

```py
    if "‚ò£Ô∏è Failed" in content or "‚ùå Error" in content: #1
        st.write(content) #2
    else: #3
        summary = summarize_content(content) #4
        st.subheader("üìã Website Summary")
        st.write(summary)
```

1. Muestra la condici√≥n "if" Si el contenido de la entrada tiene un fallo o tiene un error.

2. En caso verdadero (True) mostrar√° el error en pantalla.

3. Muestra la condici√≥n "else" si no hay fallo o error en el contenido.

4. Entonces mostramos el resumen (summary) de la p√°gina web con el "subheader" (encabezado) del sitio web y el resumen del contenido (st.write(summary)).

## Listo! üéâ

Nuestra aplicaci√≥n ya est√° completa y lista para funcionar, as√≠ que este es **todo** el c√≥digo que necesitamos escribir para crear nuestro **scraper web de IA**. Espero que hayas podido seguir y entender esto, y pronto estaremos ejecutando este archivo.

Anterior --> [**Click aqu√≠**](./03a_InstalarDependencias.md)

Siguiente --> [**Click aqu√≠**](./03c_FuncionScraper.md)