# Almacenar Datos Extra√≠dos en una Base de datos Vectorial

Lo que vamos a hacer aqu√≠ es **mejorar** nuestro raspador web con estas mejoras:

- Almacenamos el contenido web extra√≠do en una *base de datos vectorial* que es **FAISS**.

- Permitimos a los usuarios *buscar en el conocimiento almacenado*.

- Recuperaremos *contenido relevante* usando IA.

As√≠ que empecemos:

1. Primero instalaremos las dependencias necesarias.

2. Modificaremos los raspadores web de IA para almacenar datos en archivos donde:

- Primero extraeremos el sitio web.

- Luego incrustaremos el contenido.

- Despu√©s lo almacenaremos en los archivos o en la base de datos vectorial (FAISS).

- Al final, habilitaremos la b√∫squeda potenciada por IA en este contenido almacenado.

---

## Archivo nuevo

Primero vamos a ir a la terminal en nuestra ruta:

*C:\Users\alumno\Desktop\AIAgents\Day3>*

En lugar de actualizar nuestra aplicaci√≥n (ai_web_scraper.py) vamos a mantenerla tal y como est√°. Crearemos un archivo nuevo y lo guardaremos como nombre "ai_web_scrapper_faiss.py". As√≠ ya podemos empezar a escribir c√≥digo.

## Dependencias necesarias

Lo primero que necesitamos hacer es asegurarnos de haber instalado todas nuestras dependencias. Para esto, tenemos que volver a la *terminal* y asegurarme de que todas nuestras dependencias est√©n instaladas. 

Esto incluye tambi√©n instalar lo que ya hemos instalado anteriormente, as√≠ que haremos un recuento de los paquetes instalados y los que vamos a instalar a continuaci√≥n:

```bash
pip install requests beautifulsoup4 langchain_ollama faiss-cpu chromadb streamlit
```

Ejecutamos el comando y, despu√©s de instalar dependencias, limpiamos la terminal (*cls* o *clear*)

## Recuento de dependencias ya instalados 

Por si no viste --> [03a_Dependencias](./03a_InstalarDependencias.md):

1. **requests** --> Obtiene datos de p√°ginas web.

2. **beautifulsoup4** --> Extrae y limpia texto de HTML.

3. **langchain_ollama** --> Usa Ollama para el *resumen impulsado con IA* (AI-powered summarization).

4. **streamlit** --> Construye una interfaz web para el *raspador web* (web scrapper).

## Dependencias a instalar para FAISS (Base de datos vectorial):

1. **faiss-cpu**: Sirve para la *b√∫squeda eficiente de similitud*, es decir que busca los vectores m√°s similares a una consulta; y *clustering de vectores densos*. Indexa grandes cantidades en embeddings de forma eficiente.

2. **chromadb**: Es una alternativa de almacenaciemto de vectores local, completa y m√°s moderna. Sirve para guardar tanto los vectores como los documentos originales y metadatos. 

---

# Empieza el c√≥digo

Ahora vamos a dirigirnos a nuestro archivo creado *ai_web_scrapper_faiss.py* y comenzaremos con nuestras importaciones:

```py
import requests # Solicitudes HTTP
from bs4 import BeautifulSoup # Obtenci√≥n de p√°ginas web
import streamlit as st # Interfaz web
import faiss # Busquedas de similitud
import numpy as np 
from langchain_huggingface import HuggingFaceEmbedding # Updated Import
from langchain_community.vectorstores import FAISS # Base de datos vectorial
from langchain.text_splitter import CharacterTextSplitter # Dividor texto
from langchain.schema import Document # Documentos
```

## Datos importantes

- **from langchain_huggingface import HuggingFaceEmbeddings**: 

Esta importaci√≥n saca una clase "HugginFaceEmbeddings" que te permite usar modelos de embedding de Hugging Face (una plataforma con miles de modelos de IA open-source), y sirve para convertir tus textos en vectores.

Esta es una importaci√≥n actualizada. Esto sol√≠a ser un poco diferente de la versi√≥n anterior.As√≠ que si ten√≠as la anterior, se obten√≠a directamente de Langchain. Ejecutar√≠amos el siguiente comando: 

```bash
pip install langchain_huggingface
```

Actualiza el archivo y ver√°s c√≥mo ya funciona.

- **from langchain.text_splitter import CharacterTextSplitter**: 

Es el dividor de texto de caracteres de LangChain. Divide textos largos en fragmentos m√°s peque√±os.

Este m√≥dulo proviene de la versi√≥n anterior de LangChain 0.2.x. Si no te importa el archivo, prueba a importar con este otro:

```py
from langchain_text_splitters import CharacterTextSplitter
```

Si eso no funciona, prueba a instalar el siguiente paquete desde la terminal:

```bash
pip install langchain-text-splitters
```

- **from langchain.schema import Document**: 

Representa un documento o fragmento de texto estructurado para el procesamiento en LangChain. Este tambi√©n proviene de la versi√≥n anterior de LangChain 0.2.x. Si hay fallo en la importaci√≥n, prueba a importar con este otro:

```py
from langchain_core.documents import Document
```

Si eso no funciona, asegurate de instalar el siguiente paquete desde la terminal:

```bash
pip install langchain-core
```

---

## Modelo de IA

Ahora vamos a cargar el modelo de IA:

```py
# Load AI Model
llm = OllamaLLM(model="mistral") # Change to "llama3" or another Ollama model
```

## Hugging Face Embeddings

A continuaci√≥n, cargaremos las incrustaciones de la p√°gina de Hugging:

```py
# Load Hugging Face Embeddings (Update)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

Esta es una versi√≥n actualizada de LangChain en comparaci√≥n con la versi√≥n anterior de la V.0.2.x. Esto carga un modelo de transformador de oraciones (all-MiniLM-L6-v2).

Esto se utiliza para convertir texto en incrustaciones num√©ricas para la b√∫squeda de similitud. Y eso es lo que necesitamos en una base de datos vectorial.

## FAISS Vector Database

A continuaci√≥n, vamos a proceder a inicializar un almac√©n de vectores de FAISS:

```py
# Initialize FAISS Vector Database
index = faiss.IndexFlatL2(384) # Vector dimension for MiniLM
vector_store = {}
```

El *"index"* crea un √≠ndice de archivos para la b√∫squeda de similitud. La *"IndexFlatL2"*, que es la distancia euclidiana, se utiliza para medir la similitud. El n√∫mero *"384"* es el tama√±o del vector que coincide con el modelo de incrustaci√≥n MiniLM.

La dimensi√≥n del vector para el almac√©n de vectores miniLM est√° vac√≠a. Por eso, hemos creado un almac√©n de vectores (*"vector_store"*) que es un diccionario para guardar URLs y verificaciones.

## Funci√≥n para extraer datos de un sitio web

Ahora vamos a escribir una funci√≥n para extraer datos de un sitio web:

```py
# Function to scrape a website
def scrape_website(url): #1
    try: #2
        st.write(f"üåç Scraping website: {url}") #3
        headers = {"User-Agent": "Mozilla/5.0"} #4
        response = requests.get(url, headers=headers) #5

        if response.status_code != 200: #6
            return f"‚ò£Ô∏è Failed to fetch {url}" #7
        
        # Extract text content
        soup = BeautifulSoup(response.text, "html.parser") #8
        paragraphs = soup.find_all("p") #9
        text = " ".join([p.get_text() for p in paragraphs]) #10

        return text[:5000] #11
    except Exception as e: #12
        return f"‚ùå Error: {str(e)}" #13
        
```

Entre el punto 1 y 13 te lo explica en el siguiente enlace --> [**03b_WebScraper**](./03b_WebScraper.md)

Esta vez vamos a devolver un texto que va a utilizar *5000* caracteres para evitar un procesamiento excesivo.

## Funci√≥n para almacenar datos en FAISS

A continuaci√≥n, vamos a escribir la funci√≥n para almacenar datos en FAISS:

```py
# Function to store data in FAISS
def store_in_faiss(text, url): #1
    global index, vector_store #2
    st.write("üì© Storing data in FAISS...") #3

    # Split text into chunks
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100) #4
    texts = splitter.split_text(text) #5

    # Convert text into embeddings
    vectors = embeddings.embed_documents(texts) #6
    vectors = np.array(vectors, dtype=np.float32) #7

    # Store in FAISS
    index.add(vectors) #8
    vector_store[len(vector_store)] = (url, texts) #9

    return "‚úÖ Data stored successfully!" #10
```

1. Creamos la funci√≥n "store_in_faiss" con sus par√°metros "text" y "url".

2. Esto declara "index" y "vector_store" como *variables globales* para modificarlas fuera de esta funci√≥n tambi√©n.

3. Mostramos un mensaje en *streamlit* diciendo "Almacenando datos en FAISS...".

4. Ah√≠ se empieza a dividir el texto en fragmentos. Esta l√≠nea divide el texto en fragmentos de 500 caracteres (chunk_size) con una superposici√≥n de 100 caracteres (chunk_overlap).

5. Esto divide completamente el texto en fragmentos de 500 caracteres basado en la l√≠nea anterior.

6. Ah√≠ empezamos a convertir texto en incrustaciones. Esta l√≠nea hace convertir los fragmentos de texto en incrustaciones usando el modelo de Hugging Face.

7. Despu√©s convierte las incrustaciones en un array de numpy en formato float32 (dtype=np.float32).

8. Vamos a almacenar las incrustaciones creadas en archivos a√±adiendolos al "index".

9. Mediremos la longitud de los datos incrustados con la url y los textos.

10. Devolvemos los datos con √©xito.

# Funci√≥n para recuperar fragmentos relevantes y responder preguntas

A continuaci√≥n vamos a escribir una funci√≥n para recuperar fragmentos relevantes y sus preguntas:

```py
# Function to retrieve relevant chunks and answer questions
def retrieve_and_answer(query): #1
    global index, vector_store #2

    # Convert query into embedding
    query_vector = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1) #3

    # Search FAISS
    D, I = index.search(query_vector, k=2) #4

    context = "" #5
    for idx in I[0]: #6
        if idx in vector_store: #7
            context += " ".join(vector_store[idx][1]) + "\n\n" #8
    
    if not context: #9
        return "üì© No relevant data found." #10

    # Ask AI to generate an answer
    return llm.invoke(f"Based on the following context, answer the question:\n\n{context}\n\n Question: {query}\nAnswer:") #11
```

1. Creamos la funci√≥n "store_in_faiss" con su par√°metro de consulta "query". Esto define una funci√≥n para recuperar fragmentos de texto relevantes de archivos.

2. Como vimos en la funci√≥n anterior, declaramos "index" y "vector_store" en *variables globales*.

3. Corvierte la consulta del usuario en una incrustaci√≥n en el array de numpy, de tipo "float32", y remodela eso a un vector de 1, -1.

4. Ah√≠ procedemos a buscar los archivos por el √≠ndice (index). Lee dos fragmentos similares (k=2), por lo cual busca los dos fragmentos de texto m√°s relevantes.

5. Declaramos variable "context" con un texto vac√≠o.

6. Iniciamos bucle "for" declarando variable "idx" para mostrar incrustaci√≥n indicando "I" desde el punto 0 del array.

7. Condici√≥n Si "idx" se encuentra en el almacenamiento del vector (vector_store).

8. En caso verdadero, a√±ade el contenido del vector en el "context". 

As√≠ que esta funci√≥n recupera los fragmentos de texto que coinciden con la consulta y los adjunta en "context".

9. Condici√≥n Si no hay contexto.

10. En caso verdadero, devuelve el mensaje "No se enconr√≥ datos relevantes.". En caso falso, se seguir√° con la funci√≥n.

11. Pidamos a la IA que genere una respuesta as√≠ que devolvemos la repuesta de la IA con el modelo LLM. Si hay un contexto y una pregunta, ser√° la consulta que se pasa por "invoke". As√≠ que eso proceder√° a recuperar datos de la base de datos *FAISS* y los devolver√° a quien lo haya solicitado.

# Interfaz web de Streamlit

Vamos a crear nuestra interfaz web de Streamlit:

```py
# Streamlit Web UI
st.title("ü§ñ AI-Powered Web Scraper with FAISS Storage") #1
st.write("‚õìÔ∏è‚Äçüí• Enter a website URL below and store its knowledge for AI-based Q&A!") #2

# User input for website
url = st.text_input("‚õìÔ∏è‚Äçüí• Enter Website URL:") #3
if url: #4
    content = scrape_website(url) #5

    if "‚ò£Ô∏è Failed" in content or "‚ùå Error" in content: #6
        st.write(content) #7
    else: #8
        store_message = store_in_faiss(content, url) #9
        st.write(store_message)#10

# User input for Q&A
query = st.text_input("‚ùì Ask a question based on stored content:") #11
if query: #12
    answer = retrieve_and_answer(query) #13
    st.subheader("ü§ñ AI Answer:") #14
    st.write(answer) #15
```

1. A√±adimos un titulo de Streamlit que dice "Raspador web potenciado por IA con Almacenamiento FAISS"

2. Damos una instrucci√≥n de texto. "Introduzca abajo una URL de sitio web y guarde para IA preguntas y respuestas". 

3. Este es el campo de entrada de texto donde ingrese la URL del sitio web.

4. Condici√≥n Si el usuario ingresa una URL.

5. En caso verdadero, el contenido es *raspar la URL del sitio web*. Esto crea un cuadro de entrada de texto donde el usuario ingresa una URL de sitio web y llama a la funci√≥n "scrape_website" para extraer contenido.

6. Condici√≥n Si hay un fallo o un error en la URL.

7. En caso verdadero, dar√° el mensaje de fallo o error basado en lo que obtenemos.

8. Condici√≥n else si no hay fallo o error.

9. Si tiene √©xito, almacenar√° el mensaje, en archivos con el contenido de la URL.

10. Luego muestra el mensaje al usuario ah√≠.

11. A continuaci√≥n, sigamos adelante y obtengamos la entrada del usuario para qna (preguntas y respuestas). As√≠ que declaramos "query" para hacer la pregunta basada en el contenido almacenado.

12. Condici√≥n Si hay consulta.

13. En caso verdadero, muestra la respuesta (answer) para recuperar una respuesta basada en la consulta que el usuario ha ingresado.

14. Subencabezado con respuesta IA.

15. Respuesta HTTP. La respuesta que recuperamos la voy a mostrar al usuario.

--- 

As√≠ que esa es **toda la aplicaci√≥n** que no solo extrae el sitio web, sino que tambi√©n lo almacena localmente, y luego te da la opci√≥n de hacer preguntas basadas en el contenido almacenado.

## Ejecuci√≥n de la aplicaci√≥n

El siguiente paso a seguir es ejecutar la aplicaci√≥n. As√≠ que nos dirigimos a la terminal situandonos en la ruta donde se encuentra la aplicaci√≥n:

*C:\Users\alumno\Desktop\AIAgents\Day3>*

Una vez dentro, ejeutaremos la interfaz web con *Streamlit*:

```bash
streamlit run ai_web_scrapper_faiss.py
```

Una vez ejecutado, nos saldr√≠a este error en el navegador:

![RespuestaIA](./img/response_faiss01.png)

Esto se debe a que todav√≠a no hemos instalado los transformadores de sentencia para convertir el texto en incrustaciones. As√≠ que hacemos lo que nos pide el mensaje y ejecutamos el siguiente comando:

```bash
pip install sentence-transformers
```

Una vez instalado, volveremos a ejecutar la interfaz web y como resultado nos saldr√≠a as√≠:

![RespuestaIA](./img/response_faiss02.png)

Para m√≠ se me ha abierto el navegador, y si no te salta el navegador copia el enlace "localhost:8501" o "x.x.x.x:8501" (x marca los n√∫meros de tu direcci√≥n IP) y ejec√∫tala luego en el navegador.

En la aplicaci√≥n nos pide una URL, as√≠ que podemos usar el enlace de la wikipedia (https://es.wikipedia.org/wiki/Inteligencia_artificial) y ejecutarlo presionando Enter. 

![RespuestaIA](./img/response_faiss03.png)

Ahora entramos en el campo de preguntas y preguntamos: "What is IA?"

![RespuestaIA](./img/response_faiss04.png)

Y como resultado nos saldr√≠a el resultado de la IA:

![RespuestaIA](./img/response_faiss05.png)

Y me dio la respuesta basada en lo que obtuvo de ese almac√©n de datos que guardamos aqu√≠.

As√≠ que, si tambi√©n intento buscar estas cosas, problablemente lo encontrar√© en esta URL de Wikipedia aqu√≠.

As√≠ que espero que hayas podido seguir y hacer que esto funcione tanto para construir un simple raspador web de IA con la interfaz de usuario como para construirlo y almacenarlo dentro de una base de datos vectorial.

Anterior p√°gina: Ejecutar Web Scrapper --> [**Click aqu√≠**](./03d_ExeWebScraperIA.md)

Pasar al d√≠a 4 --> [**Click aqu√≠**](../Day4/04_Intro.md)