# Construir el Lector de Documentos de IA

Entonces, con todo listo, avancemos y construyamos el lector de documentos de IA.

Crearemos una *IA* que:

- Suba y lea *PDFs*.

- Almacene el texto extra√≠do en *FAISS*.

- Permita a los usuarios *hacer preguntas* basadas en el conocimiento almacenado.

As√≠ que avancemos a nuestro proyecto donde creamos el d√≠a 4:

*C:\Users\alumno\Desktop\AIAgents\Day4>*

Voy a crear un nuevo archivo aqu√≠, le llamaremos "ai_document_reader.py".

Ese ser√° nuestro archivo de Python, as√≠ que empezaremos a escrivir c√≥digo.

## Importaci√≥n de bibliotecas instaladas

Aqu√≠ vamos a importar las bibliotecas necesarias para hacer funcionar la aplicaci√≥n:

```py
import streamlit as st #1
import faiss #2
import numpy as np #3
import pypdf #4
from langchain_ollama import OllamaLLM #5
from langchain_huggingface import HuggingFaceEmbeddings #6
from langchain_community.vectorstores import FAISS #7
from langchain_text_splitters import CharacterTextSplitter #8
from langchain_core.documents import Document #9
```

1. **import streamlit as st**: Se utiliza para construir una interfaz de usuario basada en la web para subir PDFs e interactuar con el sistema de preguntas y respuestas impulsado por IA en este caso particular.

2. **import faiss**: Biblioteca para la b√∫squeda eficiente de similitudes utilizada aqu√≠ para almacenar y recuperar incrustaciones vectoriales. La raz√≥n por la que se llama "FAISS" entre iniciales es "Facebook AI Similarity Search".

3. **import numpy as np**: Se utiliza para manejar c√°lculos num√©ricos, especialmente para convertir incrustaciones de texto en un formato compatible con FAISS.

4. **import pypdf**: Biblioteca para extraer texto de archivos PDF.

5. **from langchain_ollama import OllamaLLM**: Se utiliza para cargar e interactuar con el modelo de Ollama como "mistral" o "llama3".

6. **from langchain_huggingface import HuggingFaceEmbeddings**: Esto maneja la generaci√≥n de incrustaciones de texto usando los transformadores de oraciones de Huggingface.

7. **from langchain_community.vectorstores import FAISS**: Esto proporciona la integraci√≥n de FAISS en Langchain para una recuperaci√≥n de documentos basada en vectores de manera eficiente.

8. **from langchain_text_splitters import CharacterTextSplitter**: Esto divide el texto en fragmentos m√°s peque√±os para facilitar un mejor indexado y recuperaci√≥n.

9. **from langchain_core.documents import Document**: Esto define el esquema del documento para el procesamiento de texto.

## Modelo de IA

A continuaci√≥n vamos a cargar nuestro modelo de IA:

```py
# Load AI Model
llm = OllamaLLM(model="mistral") # Change to "llama3" or another Ollama model
```

Como se dijeron en anteriores d√≠as, se puede cambiar a cualquiera de los modelos disponibles en el sitio web de Ollama (deepseek, llama3...).

## Embeddings de Hugging Face

A continuaci√≥n, vamos a continuar con las incrustaciones (embeddings) de texto:

```py
# Load Hugging Face Embeddings 
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

Como hemos visto antes en el d√≠a 3, se utiliza para convertir texto en incrustaciones num√©ricas para la b√∫squeda de similitud. Y eso es lo que necesitamos en una base de datos vectorial.

Estos embeddings ayudan a almacenar y recuperar fragmentos de texto relevantes usando FAISS. As√≠ que, a continuaci√≥n:

## Base de datos vectorial FAISS

Vamos a continuar e inicializar la base de datos de vectores FAISS:

```py
# Initialize FAISS Vector Database
index = faiss.IndexFlatL2(384) # Vector dimension for MiniLM
vector_store = {}
```

Este "faiss.indexFlatL2" inicializa un √≠ndice FAISS para almacenar vectores. "384" es el tama√±o de incrustaci√≥n (embedding) de "MiniLM-L6-v2", asegurando compatibilidad. La distancia "L2", que es la distancia euclidiana, se utiliza para la b√∫squeda de similitud.

Como un almac√©n de vectores (vector_store), que es un diccionario para almacenar documentos, metadatos y fragmentos de texto.

## Funci√≥n extract_text_from_pdf

Ahora vamos a continuar y escribir una funci√≥n para *extraer texto de un PDF*:

```py
# Function to extract text from PDFs
def extract_text_from_pdf(uploaded_file): #1
    pdf_reader = pypdf.PdfReader(uploaded_file) #2
    text = "" #3
    for page in pdf_reader.pages: #4
        text += page.extract_text() + "\n" #5
    return text #6
```

1. Llamamos a la funci√≥n "extract_text_from_pdf" y recibir√° un archivo subido (uploaded_file) como entrada.

2. "pdf_reader" lee un archivo PDF subido.

3. Declaramos "text" como un campo vac√≠o.

4. Esto dice: Para cada p√°gina en "pdf_reader.pages" esto iterar√° a trav√©s de todas las p√°ginas y extraer√° texto.

5. As√≠ que aqu√≠ voy a obtener el texto, a√±adirlo diciendo "page.extract_text()" y a√±adir salto de l√≠nea (\n). Tambi√©n concatena el texto en una sola cadena.

6. Devolvemos el texto extra√≠do.

## Funci√≥n store_in_faiss

Ahora vamos a escribir una funci√≥n para *almacenar texto en FAISS*:

```py
# Function to store text in FAISS
def store_in_faiss(text, filename): #1
    global index, vector_store #2
    st.write(f"üì© Storing document '{filename}' in FAISS...") #3

    # Split text into chunks
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100) #4
    texts = splitter.split_text(text) #5

    # Convert text into embeddings
    vectors = embeddings.embed_documents(texts) #6
    vectors = np.array(vectors, dtype=np.float32) #7 

    # Store in FAISS
    index.add(vectors) #8
    vector_store[len(vector_store)] = (filename, texts) #9

    return "‚úÖ Document stored successfully!" #10

```

1. Llamamos a la funci√≥n "store_in_faiss" y toma como entrada el texto (text) y el nombre del archivo (filename).

2. Vamos a crear un uso global, √≠ndice global (index) y almac√©n de vectores (vector_store) para almacenar el documento.

3. Mensaje de Streamlit diciendo: Almacenando documento {nombre_archivo} en FAISS..."

4. Dividimos el texto en fragmentos. Pondremos el tama√±o de fragmento del splitter a 500 car√°cteres, y superposici√≥n de fragmento a 100 car√°cteres. La superposici√≥n asegura que no se pierda informaci√≥n importante entre los fragmentos.

5. Esto divide el texto en fragmentos m√°s peque√±os de 500 car√°cteres con una superposici√≥n de 100 car√°cteres.

6. Ah√≠ empezamos a convertir texto en incrustaciones. Esta l√≠nea hace convertir los fragmentos de texto en incrustaciones usando el modelo de Hugging Face.

7. Despu√©s convierte las incrustaciones en un array de numpy en formato float32 (dtype=np.float32).ectores 

8. Empezaremos a almacenar en FAISS. As√≠ que diremos "index" que a√±ada vectores, que lo guardar√° como un almac√©n de vectores.

9. Mediremos la longitud de los datos incrustados con el nombre del archivo y los textos.

10. Devolveremos una respuesta de que el documento se almacen√≥ con √©xito.

## Funci√≥n retrieve_and_answer

Vamos a escribir la funci√≥n para *recuperar fragmentos y responder preguntas*:

```py
# Function to retrieve relevant chunks and answer questions
def retrieve_and_answer(query): #1
    global index, vector_store #2

    # Convert query into embeddings
    query_vector = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1) #3

    # Search FAISS
    D, I = index.search(query_vector, k=2) # Retrieve top 2 similar chunks #4

    context = "" #5
    for idx in I[0]: #6
        if idx in vector_store: #7
            context += " ".join(vector_store[idx][1]) + "\n\n" #8

    if not context: #9
        return "ü§ñ No relevant data found in stores documents." #10

    # Ask AI to generate an answer
    return llm.invoke(f"Based on the following document context, answer the question:\n\n{context}\n\nQuestion: {query}\nAnswer:") #11
```

1. Llamaremos a la funci√≥n "retrieve_and_answer" y toma como entrada el √≠ndice global de consultas (query). Esto tomar√° la consulta (query) de un usuario y recuperar√° fragmentos de texto relevantes de los documentos almacenados.

2. Usaremos el uso global "index" y "vector_store".

3. Procederemos a convertir una consulta en incrustaci√≥n, empezando por el vector de consulta (query_vector). Convierte la consulta del usuario en una representaci√≥n vectorial y la reformatea par asegurar la compatibilidad con los archivos.

4. Una vez que se ejecuta el vector de consulta, lo siguiente es proceder a buscar en los archivos. Esta l√≠nea busca en los archivos los dos fragmentos de texto m√°s relevantes, por eso "k=2" muestran los dos vecinos m√°s cercanos principales.

5. Pondremos contexto igual a texto vac√≠o.

6. Esta l√≠nea es para recuperar los fragmentos coincidentes para el √≠ndice.

7. Condici√≥n si "idx" est√° en "vector_store".

8. En cso verdadero, esta l√≠nea extrae fragmentos de documentos relevantes basados en los √≠ndices recuperados, y luego los concatena en una cadena de contexto.

9. Vamos a manejar el caso cuando no se encuentra ning√∫n dato relevante.

10. Si no hay contexto, nos devuelve un mensaje que no se encontraron datos relevantes en los documentos almacenados.

11. Aqu√≠ generamos la respuesta de la IA. Le pediremos que genere una respuesta a la IA. As√≠ que devuelve la invocaci√≥n del modelo de IA que env√≠a el contexto del documento recuperado y la consulta del usuario al modelo de IA o Ollama LLM, y el modelo genera una respuesta basada en el texto recuperado.

## Interfaz UI (Web UI)

Vamos a continuar y comenzar con nuestra interfaz web de Streamlit:

```py
# Streamlit Web UI
st.title("üìã AI Document Reader & Q&A Bot") #1
st.write("Upload a PDF and ask questions based on its content!") #2

# File uploader for PDF
uploaded_file = st.file_uploader("üìÇ Upload a PDF Document", type=["pdf"]) #3
if uploaded_file: #4
    text = extract_text_from_pdf(uploaded_file) #5
    store_message = store_in_faiss(text, uploaded_file.name) #6
    st.write(store_message) #7

# User input for Q&A
query = st.text_input("‚ùì Ask a question based on the uploaded document:") #8
if query: #9
    answer = retrieve_and_answer(query) #10
    st.subheader("ü§ñ AI Answer:") #11
    st.write(answer) #12
```

1. Empezaremos con el t√≠tulo HTML que dice "Lector de documentos IA y Bot de preguntas y respuestas".

2. A continuaci√≥n escibimos mensaje que dice "Sube un PDF y haz preguntas basadas en su contexto".

Esto configura el t√≠tulo b√°sico de la interfaz y la descripci√≥n de la aplicaci√≥n.

3. Vamos a continuar creando el cargador de archivos para PDF. En esta l√≠nea diremos que subimos un documento PDF y el tipo es PDF. Esto asegura que cuando el usuario busque solo le proporcionar√±a archivos PDF.

4. Condici√≥n Si se sube el archivo PDF.

5. En caso afirmativo, llamamos a la funci√≥n para que extraiga texto del PDF.

6. Luego, mostrar√≠a un mensaje de almacenamiento llamando a la funci√≥n que almacena en FAISS.

7. Despu√©s de la funci√≥n obtendr√° el mensaje de almacenamiento.

8. A continuaci√≥n crearemos la entrada de usuario para hacer preguntas. Decimos Consulta igual a la entrada de texto para hacer una pregunta basada en el documento subido.

9. Condici√≥n Si se ha hecho una pregunta de usuario.

10. En caso afirmativo, responde con la funci√≥n que usamos para recuperar contenido y responder.

11. Escribimos subencabezado de la respuesta IA.

12. Escribimos respuesta. Proporciona un cuadro de entrada de texto para que los usuarios hagan preguntas y llaman a "recuperar y responder consulta" para obtener una respuesta y mostrarla.

## Resumen 

As√≠ que esa es toda nuestra aplicaci√≥n. Este tipo de c√≥digo carga los modelos de IA necesarios (OllamaLLM y HuggingFace). 

Divide el texto en fragmentos m√°s peque√±os y almacena los embeddings en archivos. 

Luego recupera los fragmentos relevantes basado en las consultas de los usuarios.

Pasa el texto recuperado al modelo de IA para generar una respuesta y luego proporciona una interfaz de usuario basada en Streamlit para una interacci√≥n f√°cil.

As√≠ que vamos adelante üöÄ

Antes de ejecutarlo, quiero saber m√°s sobre esta aplicaci√≥n, sobre c√≥mo usarla, as√≠ que espero que hayas podido seguir y lograr implementarlo.

Nuestro siguiente paso ser√° entender qu√© hace este lector.

Anterior p√°gina: Dependencias --> [**Click aqu√≠**](./04a_Dependencias.md)

Siguiente p√°gina: C√≥mo funciona el Lector de Documentos de IA --> [**Click aqu√≠**](./04c_FuncionLector.md)